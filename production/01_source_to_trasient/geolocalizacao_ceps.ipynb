{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9215056c-1688-4c09-9e5e-75e41f58604b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# PASSO 1: CONFIGURAÇÃO E CARGA DOS CEPS DE ORIGEM\n",
    "# =======================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "print(\"Iniciando a criação da tabela de geolocalização fictícia.\")\n",
    "\n",
    "# Nome da tabela de onde vamos ler os CEPs\n",
    "NOME_TABELA_ORIGEM = \"production.transient.pms_hoteis\"\n",
    "\n",
    "try:\n",
    "    # Carrega a tabela de hóspedes e seleciona apenas os CEPs únicos e não nulos\n",
    "    df_ceps = spark.table(NOME_TABELA_ORIGEM) \\\n",
    "        .select(\"cep\") \\\n",
    "        .filter(F.col(\"cep\").isNotNull()) \\\n",
    "        .distinct()\n",
    "    \n",
    "    print(f\"Encontrados {df_ceps.count()} CEPs únicos na tabela '{NOME_TABELA_ORIGEM}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar a tabela '{NOME_TABELA_ORIGEM}': {e}\")\n",
    "    dbutils.notebook.exit(\"Verifique se a tabela de hóspedes existe antes de executar este script.\")\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 2: GERAÇÃO ALEATÓRIA DE LATITUDE E LONGITUDE\n",
    "# =======================================================================\n",
    "\n",
    "# Limites geográficos aproximados do Brasil (bounding box)\n",
    "LAT_MIN, LAT_MAX = -33.75, 5.27  # Do Sul ao Norte\n",
    "LON_MIN, LON_MAX = -73.98, -34.79 # Do Oeste ao Leste\n",
    "\n",
    "print(\"Gerando coordenadas aleatórias dentro dos limites do Brasil...\")\n",
    "\n",
    "# Usamos a função nativa F.rand() do Spark, que gera um número aleatório entre 0 e 1.\n",
    "# Em seguida, ajustamos a escala desse número para que ele se encaixe no nosso intervalo de lat/lon.\n",
    "# Esta abordagem é 100% executada no motor do Spark, sendo extremamente performática.\n",
    "df_geolocalizacao_ficticia = df_ceps.withColumn(\n",
    "    \"latitude\", \n",
    "    (F.rand() * (LAT_MAX - LAT_MIN) + LAT_MIN).cast(DoubleType())\n",
    ").withColumn(\n",
    "    \"longitude\", \n",
    "    (F.rand() * (LON_MAX - LON_MIN) + LON_MIN).cast(DoubleType())\n",
    ")\n",
    "\n",
    "print(\"Coordenadas fictícias geradas com sucesso.\")\n",
    "display(df_geolocalizacao_ficticia.limit(10))\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 3: SALVAR A TABELA DELTA FINAL\n",
    "# =======================================================================\n",
    "\n",
    "# Você pode alterar \"nome_tabela\" para o nome que preferir\n",
    "NOME_TABELA_FINAL = \"production.transient.api_geolocalizacao_ceps\"\n",
    "\n",
    "print(f\"\\nSalvando a tabela de referência em: {NOME_TABELA_FINAL}\")\n",
    "\n",
    "df_geolocalizacao_ficticia.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(NOME_TABELA_FINAL)\n",
    "\n",
    "print(\"Tabela salva com sucesso!\")\n",
    "\n",
    "# Otimiza a tabela para consultas rápidas por CEP\n",
    "print(\"Otimizando a tabela...\")\n",
    "spark.sql(f\"OPTIMIZE {NOME_TABELA_FINAL} ZORDER BY (cep)\")\n",
    "print(\"Otimização concluída.\")\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 4: VERIFICAÇÃO\n",
    "# =======================================================================\n",
    "df_verificacao = spark.table(NOME_TABELA_FINAL)\n",
    "print(f\"\\nVerificação: a tabela '{NOME_TABELA_FINAL}' foi criada com {df_verificacao.count()} registros.\")\n",
    "display(df_verificacao.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5150682532233903,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "geolocalizacao_ceps",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
