{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce04277a-8985-42f9-8ae3-680b20eb40e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install Faker names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6230f41e-f730-47ea-bb0b-22b323c3313d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hoteis"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DateType)\n",
    "\n",
    "# Inicializa o Faker para gerar dados em português do Brasil\n",
    "fake = Faker('pt_BR')\n",
    "\n",
    "# --- Configurações e Listas de Apoio ---\n",
    "\n",
    "# Lista de comodidades que um hotel pode oferecer\n",
    "COMODIDADES_DISPONIVEIS = [\n",
    "    'Wi-Fi Grátis', 'Piscina', 'Academia', 'Spa', 'Estacionamento',\n",
    "    'Restaurante', 'Bar', 'Serviço de Quarto', 'Recepção 24h',\n",
    "    'Ar Condicionado', 'Pet Friendly', 'Salão de Eventos', 'Business Center'\n",
    "]\n",
    "\n",
    "# Listas para gerar nomes de hotéis mais realistas\n",
    "PREFIXOS_NOMES = ['Grand', 'Royal', 'Plaza', 'Imperial', 'Golden', 'Paradise', 'Blue Tree']\n",
    "SUFIXOS_NOMES = ['Hotel', 'Resort', 'Palace', 'Inn', 'Pousada', 'Tower']\n",
    "\n",
    "\n",
    "def gerar_data_abertura():\n",
    "    \"\"\"Gera uma data de abertura aleatória entre 1990 e 2020.\"\"\"\n",
    "    data_inicio = datetime(1990, 1, 1)\n",
    "    data_fim = datetime(2020, 12, 31)\n",
    "    diferenca_dias = (data_fim - data_inicio).days\n",
    "    dias_aleatorios = random.randint(0, diferenca_dias)\n",
    "    return data_inicio + timedelta(days=dias_aleatorios)\n",
    "\n",
    "def gerar_hoteis(total_hoteis=50):\n",
    "    \"\"\"\n",
    "    Gera uma lista de dados fictícios de hotéis.\n",
    "\n",
    "    Args:\n",
    "        total_hoteis (int): O número total de hotéis a serem gerados.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de tuplas, onde cada tupla representa um hotel.\n",
    "    \"\"\"\n",
    "    hoteis = []\n",
    "    for i in range(total_hoteis):\n",
    "        hotel_id = 101 + i\n",
    "        \n",
    "        # Gera um nome de hotel combinando partes aleatórias\n",
    "        nome_hotel = f\"{random.choice(PREFIXOS_NOMES)} {fake.city_suffix()} {random.choice(SUFIXOS_NOMES)}\"\n",
    "        \n",
    "        cidade = fake.city()\n",
    "        estado = fake.state_abbr()\n",
    "        \n",
    "        # Seleciona um número aleatório de comodidades da lista\n",
    "        num_comodidades = random.randint(4, 10)\n",
    "        comodidades_selecionadas = random.sample(COMODIDADES_DISPONIVEIS, num_comodidades)\n",
    "        \n",
    "        hotel = (\n",
    "            hotel_id,\n",
    "            nome_hotel,\n",
    "            fake.street_address(),\n",
    "            cidade,\n",
    "            estado,\n",
    "            fake.postcode(),\n",
    "            'Brasil',\n",
    "            random.randint(3, 5),  # Estrelas\n",
    "            random.randint(50, 400), # Número de quartos\n",
    "            ', '.join(comodidades_selecionadas), # Comodidades como string\n",
    "            gerar_data_abertura(),\n",
    "            fake.phone_number(),\n",
    "            f\"contato@{nome_hotel.lower().replace(' ', '')}.com\",\n",
    "            fake.name() # Nome do gerente\n",
    "        )\n",
    "        hoteis.append(hotel)\n",
    "        \n",
    "    return hoteis\n",
    "\n",
    "# --- Execução do Script ---\n",
    "\n",
    "# Em um ambiente como o Databricks, a sessão Spark 'spark' já vem iniciada.\n",
    "# Caso contrário, você a inicializaria assim:\n",
    "# spark = SparkSession.builder.appName(\"GeracaoHoteis\").getOrCreate()\n",
    "\n",
    "# Define o schema do DataFrame para garantir os tipos de dados corretos\n",
    "schema_hoteis = StructType([\n",
    "    StructField(\"hotel_id\", IntegerType(), False),\n",
    "    StructField(\"nome_hotel\", StringType(), True),\n",
    "    StructField(\"endereco\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cep\", StringType(), True),\n",
    "    StructField(\"pais\", StringType(), True),\n",
    "    StructField(\"estrelas\", IntegerType(), True),\n",
    "    StructField(\"numero_quartos\", IntegerType(), True),\n",
    "    StructField(\"comodidades\", StringType(), True),\n",
    "    StructField(\"data_abertura\", DateType(), True),\n",
    "    StructField(\"telefone\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gerente\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Gera a lista de dados dos hotéis\n",
    "dados_hoteis = gerar_hoteis(150)\n",
    "\n",
    "# Cria o DataFrame Spark a partir dos dados e schema definidos\n",
    "df_hoteis = spark.createDataFrame(dados_hoteis, schema_hoteis)\n",
    "\n",
    "# Mostra uma amostra dos dados gerados para verificação\n",
    "print(\"Amostra dos dados de hotéis gerados:\")\n",
    "df_hoteis.show(10, truncate=False)\n",
    "\n",
    "# Contagem de hotéis por estado para análise\n",
    "print(\"\\nContagem de hotéis por estado:\")\n",
    "df_hoteis.groupBy(\"estado\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "\n",
    "# Define o caminho de destino no catálogo do Databricks\n",
    "catalog_path = \"production.transient.pms_hoteis\"\n",
    "\n",
    "# Salva o DataFrame como uma tabela Delta, sobrescrevendo se já existir\n",
    "df_hoteis.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(catalog_path)\n",
    "\n",
    "print(f\"\\nTabela de hotéis salva com sucesso em: {catalog_path}\")\n",
    "print(f\"Total de registros gravados: {df_hoteis.count()}\")\n",
    "\n",
    "# Otimiza a tabela para melhorar o desempenho de futuras consultas\n",
    "print(\"\\nAplicando OPTIMIZE na tabela...\")\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog_path}\n",
    "ZORDER BY (estado, estrelas)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Otimização concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa65e6b9-2d09-4acb-9db0-0cde7c4d47a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "               select * from production.transient.erp_faturas\n",
    "               \"\"\")\n",
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bbf68c-a8c7-4616-8b33-61526ca774f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Consumos"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DoubleType, DateType)\n",
    "\n",
    "# Inicializa o Faker para gerar dados em português do Brasil\n",
    "fake = Faker('pt_BR')\n",
    "\n",
    "# --- DADOS DE FORMAS DE PAGAMENTO ADICIONADOS ---\n",
    "formas_pagamento_data = [\n",
    "    (\"Cartão de Crédito Visa\", \"Crédito\"),\n",
    "    (\"Cartão de Crédito Mastercard\", \"Crédito\"),\n",
    "    (\"Cartão de Crédito Amex\", \"Crédito\"),\n",
    "    (\"Cartão de Débito Visa\", \"Débito\"),\n",
    "    (\"Cartão de Débito Mastercard\", \"Débito\"),\n",
    "    (\"Pix\", \"Transferência\"),\n",
    "    (\"Dinheiro\", \"Dinheiro\"),\n",
    "    (\"Faturado (Empresa)\", \"Outros\")\n",
    "]\n",
    "\n",
    "# --- Dicionário de Serviços e Preços ---\n",
    "SERVICOS_HOTEL = {\n",
    "    'Restaurante': {\n",
    "        'itens': [\n",
    "            {'descricao': 'Café da manhã', 'valor_min': 25.00, 'valor_max': 45.00},\n",
    "            {'descricao': 'Almoço executivo', 'valor_min': 45.00, 'valor_max': 75.00},\n",
    "            # ... (demais itens)\n",
    "        ]\n",
    "    },\n",
    "    # ... (demais serviços)\n",
    "}\n",
    "\n",
    "def gerar_data_consumo():\n",
    "    \"\"\"Gera uma data de consumo aleatória dos últimos 4 anos até hoje.\"\"\"\n",
    "    data_inicio = datetime.now() - timedelta(days=4*365)\n",
    "    data_fim = datetime.now()\n",
    "    diferenca_dias = (data_fim - data_inicio).days\n",
    "    dias_aleatorios = random.randint(0, diferenca_dias)\n",
    "    return data_inicio + timedelta(days=dias_aleatorios)\n",
    "\n",
    "def gerar_consumo_servico(comodidades_hotel):\n",
    "    \"\"\"\n",
    "    Gera um consumo de serviço aleatório.\n",
    "    \"\"\"\n",
    "    servicos_disponiveis = [s for s in SERVICOS_HOTEL.keys() if s in comodidades_hotel]\n",
    "    if not servicos_disponiveis:\n",
    "        return None, None, None, None\n",
    "    tipo_servico = random.choice(servicos_disponiveis)\n",
    "    item = random.choice(SERVICOS_HOTEL[tipo_servico]['itens'])\n",
    "    valor = round(random.uniform(item['valor_min'], item['valor_max']), 2)\n",
    "    pago = random.choices(['Sim', 'Não'], weights=[85, 15])[0]\n",
    "    return tipo_servico, item['descricao'], valor, pago\n",
    "\n",
    "# --- FUNÇÃO DE GERAÇÃO ATUALIZADA ---\n",
    "def gerar_consumos(hoteis_info, total_consumos=25000):\n",
    "    \"\"\"\n",
    "    Gera uma lista de consumos dos hóspedes, associando cada consumo\n",
    "    a um hotel existente e incluindo a forma de pagamento.\n",
    "    \"\"\"\n",
    "    consumos = []\n",
    "    hospede_ids = list(range(1, 8001))\n",
    "    reserva_ids = list(range(10001, 50001))\n",
    "\n",
    "    for i in range(total_consumos):\n",
    "        hotel = random.choice(hoteis_info)\n",
    "        tipo_servico, descricao, valor, pago = gerar_consumo_servico(hotel['comodidades'])\n",
    "        \n",
    "        # --- LÓGICA PARA PAGAMENTO ADICIONADA ---\n",
    "        # Escolhe uma forma de pagamento aleatória da lista\n",
    "        forma_pagamento, tipo_pagamento = random.choice(formas_pagamento_data)\n",
    "        # --- FIM DA LÓGICA ADICIONADA ---\n",
    "\n",
    "        if tipo_servico is None:\n",
    "            continue\n",
    "            \n",
    "        consumo = (\n",
    "            5001 + i,\n",
    "            random.choice(reserva_ids),\n",
    "            random.choice(hospede_ids),\n",
    "            hotel['hotel_id'],\n",
    "            random.randint(101, hotel['numero_quartos'] + 100),\n",
    "            gerar_data_consumo(),\n",
    "            tipo_servico,\n",
    "            descricao,\n",
    "            valor,\n",
    "            pago,\n",
    "            # --- NOVAS COLUNAS ADICIONADAS AO REGISTRO ---\n",
    "            forma_pagamento,\n",
    "            tipo_pagamento\n",
    "        )\n",
    "        consumos.append(consumo)\n",
    "    \n",
    "    return consumos\n",
    "\n",
    "# --- Execução do Script ---\n",
    "\n",
    "# 1. Carregar a tabela de hotéis\n",
    "try:\n",
    "    df_hoteis_base = spark.table(\"production.transient.pms_hoteis\")\n",
    "    hoteis_info = df_hoteis_base.select(\n",
    "        col(\"hotel_id\"),\n",
    "        col(\"numero_quartos\"),\n",
    "        split(col(\"comodidades\"), \", \").alias(\"comodidades\") \n",
    "    ).collect()\n",
    "    print(f\"{len(hoteis_info)} hotéis carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar a tabela 'hoteis': {e}\")\n",
    "    dbutils.notebook.exit(\"Tabela de hotéis não encontrada.\")\n",
    "\n",
    "# 2. Definir o schema do novo DataFrame\n",
    "# --- SCHEMA ATUALIZADO ---\n",
    "schema_consumos = StructType([\n",
    "    StructField(\"consumo_id\", IntegerType(), False),\n",
    "    StructField(\"reserva_id\", IntegerType(), True),\n",
    "    StructField(\"hospede_id\", IntegerType(), True),\n",
    "    StructField(\"hotel_id\", IntegerType(), True),\n",
    "    StructField(\"quarto_id\", IntegerType(), True),\n",
    "    StructField(\"data_consumo\", DateType(), True),\n",
    "    StructField(\"tipo_servico\", StringType(), True),\n",
    "    StructField(\"descricao\", StringType(), True),\n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"pago\", StringType(), True),\n",
    "    # --- NOVOS CAMPOS ADICIONADOS AO SCHEMA ---\n",
    "    StructField(\"nome_forma_pagamento\", StringType(), True),\n",
    "    StructField(\"tipo_pagamento\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 3. Gerar os dados de consumo\n",
    "dados_consumos = gerar_consumos(hoteis_info, 25000)\n",
    "\n",
    "# 4. Criar o DataFrame Spark\n",
    "df_consumos = spark.createDataFrame(dados_consumos, schema_consumos)\n",
    "\n",
    "# 5. Análise e Verificação\n",
    "print(\"\\nAmostra dos dados de consumos gerados (com forma de pagamento):\")\n",
    "df_consumos.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nTotal de consumos por tipo de serviço:\")\n",
    "df_consumos.groupBy(\"tipo_servico\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# --- NOVA ANÁLISE DE VERIFICAÇÃO ---\n",
    "print(\"\\nTotal de consumos por tipo de pagamento:\")\n",
    "df_consumos.groupBy(\"tipo_pagamento\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "catalog_path = \"production.transient.pdv_consumos\"\n",
    "df_consumos.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(catalog_path)\n",
    "print(f\"\\nTabela de consumos salva com sucesso em: {catalog_path}\")\n",
    "\n",
    "# Otimiza a tabela\n",
    "spark.sql(f\"OPTIMIZE {catalog_path} ZORDER BY (data_consumo, hotel_id, tipo_servico)\")\n",
    "print(\"Otimização concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e66acf-1354-416c-b6cf-564d1b73dc94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Faturas"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, max, lit, rand, when\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DoubleType, DateType)\n",
    "\n",
    "# Inicializa o Faker\n",
    "fake = Faker('pt_BR')\n",
    "\n",
    "# --- Lista de Apoio ATUALIZADA ---\n",
    "FORMAS_PAGAMENTO_DATA = [\n",
    "    (\"Cartão de Crédito Visa\", \"Crédito\"),\n",
    "    (\"Cartão de Crédito Mastercard\", \"Crédito\"),\n",
    "    (\"Cartão de Crédito Amex\", \"Crédito\"),\n",
    "    (\"Cartão de Débito Visa\", \"Débito\"),\n",
    "    (\"Cartão de Débito Mastercard\", \"Débito\"),\n",
    "    (\"Pix\", \"Transferência\"),\n",
    "    (\"Dinheiro\", \"Dinheiro\"),\n",
    "    (\"Faturado (Empresa)\", \"Outros\")\n",
    "]\n",
    "\n",
    "def gerar_faturas_a_partir_de_consumos(df_consumos_agregados):\n",
    "    \"\"\"\n",
    "    Gera os detalhes da fatura para cada reserva, incluindo o tipo de pagamento.\n",
    "    \"\"\"\n",
    "    faturas = []\n",
    "    reservas_para_faturar = df_consumos_agregados.collect()\n",
    "    data_atual = datetime.now()\n",
    "\n",
    "    for i, reserva in enumerate(reservas_para_faturar):\n",
    "        fatura_id = 9001 + i\n",
    "        reserva_id = reserva['reserva_id']\n",
    "        hotel_id = reserva['hotel_id']\n",
    "        subtotal_consumos = reserva['subtotal_consumos']\n",
    "        ultima_data_consumo = reserva['ultima_data_consumo']\n",
    "\n",
    "        valor_diarias = round(random.uniform(350.0, 4500.0), 2)\n",
    "        valor_total = round(valor_diarias + subtotal_consumos, 2)\n",
    "        impostos = round(valor_total * 0.12, 2)\n",
    "        \n",
    "        data_emissao = ultima_data_consumo + timedelta(days=1)\n",
    "        data_vencimento = data_emissao + timedelta(days=random.randint(7, 20))\n",
    "        \n",
    "        if data_vencimento < data_atual.date():\n",
    "            status_pgto = random.choices(['Pago', 'Em Atraso'], weights=[85, 15])[0]\n",
    "        else:\n",
    "            status_pgto = random.choices(['Pendente', 'Pago'], weights=[70, 30])[0]\n",
    "\n",
    "        # --- LÓGICA DE PAGAMENTO ATUALIZADA ---\n",
    "        forma_pgto = None\n",
    "        tipo_pgto = None # Inicializa o tipo como nulo\n",
    "        data_pagamento = None\n",
    "        if status_pgto == 'Pago':\n",
    "            # Define pesos para uma distribuição mais realista\n",
    "            pesos_pagamento = [25, 25, 10, 10, 10, 15, 4, 1]\n",
    "            # Desempacota o nome e o tipo da forma de pagamento\n",
    "            forma_pgto, tipo_pgto = random.choices(FORMAS_PAGAMENTO_DATA, weights=pesos_pagamento, k=1)[0]\n",
    "            \n",
    "            dias_para_pagar = (min(data_vencimento, data_atual.date()) - data_emissao).days\n",
    "            data_pagamento = data_emissao + timedelta(days=random.randint(0, dias_para_pagar)) if dias_para_pagar > 0 else data_emissao\n",
    "        # --- FIM DA ATUALIZAÇÃO ---\n",
    "        \n",
    "        nota_fiscal = f\"NFE-{reserva['estado_sigla']}-{hotel_id:03d}-{reserva_id:05d}\"\n",
    "        \n",
    "        fatura = (\n",
    "            fatura_id,\n",
    "            reserva_id,\n",
    "            hotel_id,\n",
    "            data_emissao,\n",
    "            data_vencimento,\n",
    "            data_pagamento,\n",
    "            valor_diarias,\n",
    "            subtotal_consumos,\n",
    "            impostos,\n",
    "            valor_total,\n",
    "            forma_pgto,\n",
    "            tipo_pgto, # Coluna adicionada\n",
    "            status_pgto,\n",
    "            nota_fiscal\n",
    "        )\n",
    "        faturas.append(fatura)\n",
    "        \n",
    "    return faturas\n",
    "\n",
    "# --- Execução do Script ---\n",
    "\n",
    "# 1. Carregar as tabelas de dependência\n",
    "try:\n",
    "    df_consumos = spark.table(\"production.transient.pdv_consumos\")\n",
    "    df_hoteis = spark.table(\"production.transient.pms_hoteis\")\n",
    "    print(\"Tabelas 'consumos' e 'hoteis' carregadas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar tabelas: {e}\")\n",
    "    dbutils.notebook.exit(\"Tabelas de dependência não encontradas.\")\n",
    "\n",
    "# 2. Agregar os consumos por reserva\n",
    "df_consumos_agregados = df_consumos.groupBy(\"reserva_id\", \"hotel_id\") \\\n",
    "    .agg(\n",
    "        sum(\"valor\").alias(\"subtotal_consumos\"),\n",
    "        max(\"data_consumo\").alias(\"ultima_data_consumo\")\n",
    "    ) \\\n",
    "    .join(df_hoteis.select(col(\"hotel_id\"), col(\"estado\").alias(\"estado_sigla\")), \"hotel_id\")\n",
    "\n",
    "print(f\"{df_consumos_agregados.count()} reservas únicas encontradas para faturamento.\")\n",
    "\n",
    "# 3. Definir o schema do DataFrame de faturas\n",
    "# --- SCHEMA ATUALIZADO ---\n",
    "schema_faturas = StructType([\n",
    "    StructField(\"fatura_id\", IntegerType(), False),\n",
    "    StructField(\"reserva_id\", IntegerType(), True),\n",
    "    StructField(\"hotel_id\", IntegerType(), True),\n",
    "    StructField(\"data_emissao\", DateType(), True),\n",
    "    StructField(\"data_vencimento\", DateType(), True),\n",
    "    StructField(\"data_pagamento\", DateType(), True),\n",
    "    StructField(\"valor_diarias\", DoubleType(), True),\n",
    "    StructField(\"subtotal_consumos\", DoubleType(), True),\n",
    "    StructField(\"impostos\", DoubleType(), True),\n",
    "    StructField(\"valor_total\", DoubleType(), True),\n",
    "    StructField(\"forma_pagamento\", StringType(), True),\n",
    "    StructField(\"tipo_pagamento\", StringType(), True), # Coluna adicionada\n",
    "    StructField(\"status_pagamento\", StringType(), True),\n",
    "    StructField(\"nota_fiscal\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 4. Gerar os dados das faturas\n",
    "dados_faturas = gerar_faturas_a_partir_de_consumos(df_consumos_agregados)\n",
    "\n",
    "# 5. Criar o DataFrame Spark\n",
    "df_faturas = spark.createDataFrame(dados_faturas, schema_faturas)\n",
    "\n",
    "# 6. Análise e Verificação\n",
    "print(\"\\nAmostra das faturas geradas:\")\n",
    "df_faturas.select(\"fatura_id\", \"status_pagamento\", \"forma_pagamento\", \"tipo_pagamento\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\nDistribuição de faturas por tipo de pagamento:\")\n",
    "df_faturas.groupBy(\"tipo_pagamento\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "catalog_path = \"production.transient.erp_faturas\"\n",
    "df_faturas.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(catalog_path)\n",
    "print(f\"\\nTabela de faturas salva com sucesso em: {catalog_path}\")\n",
    "\n",
    "# Otimiza a tabela\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog_path}\n",
    "ZORDER BY (data_emissao, status_pagamento)\n",
    "\"\"\")\n",
    "print(\"Otimização concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61da5ade-6c92-4728-b3c3-88a13f3e9596",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hospedes"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "# Melhor Prática Definitiva: Importar functions com o alias F para evitar 100% dos conflitos de nome\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DateType)\n",
    "from faker import Faker\n",
    "\n",
    "# Configurar Faker para o Brasil\n",
    "fake = Faker('pt_BR')\n",
    "\n",
    "# --- (Suas funções e schemas permanecem os mesmos) ---\n",
    "\n",
    "def gerar_hospedes(hospedes_a_criar):\n",
    "    hospedes_completos = []\n",
    "    TIPOS_CLIENTE = ['Recorrente', 'Novo', 'VIP', 'Premium', 'Corporativo']\n",
    "    IDENTIDADES_GENERO = {\"opcoes\": [\"Mulher Cis\", \"Homem Cis\", \"Mulher Trans\", \"Homem Trans\", \"Não-binário\", \"Prefiro não informar\"], \"pesos\": [47, 46, 1, 1, 2, 3]}\n",
    "    PRONOMES = {\"opcoes\": [\"Ela/Dela\", \"Ele/Dele\", \"Elu/Delu\", \"Prefiro não informar\"], \"pesos\": [48, 47, 2, 3]}\n",
    "    ORIENTACOES_SEXUAIS = {\"opcoes\": [\"Heterossexual\", \"Bissexual\", \"Homossexual\", \"Pansexual\", \"Assexual\", \"Prefiro não informar\"], \"pesos\": [85, 4, 3, 2, 1, 5]}\n",
    "    \n",
    "    for hospede_info in hospedes_a_criar:\n",
    "        hospede_id = hospede_info['hospede_id']\n",
    "        data_ultima_estadia = hospede_info['data_ultima_estadia']\n",
    "        ultimo_hotel_id = hospede_info['ultimo_hotel_id']\n",
    "        sexo_nasc = random.choice([\"Masculino\", \"Feminino\"])\n",
    "        identidade_genero = random.choices(IDENTIDADES_GENERO[\"opcoes\"], weights=IDENTIDADES_GENERO[\"pesos\"], k=1)[0]\n",
    "        nome_registro = fake.name_male() if sexo_nasc == \"Masculino\" else fake.name_female()\n",
    "        nome_social = nome_registro\n",
    "        if (\"Trans\" in identidade_genero or \"Não-binário\" in identidade_genero) and random.random() < 0.9:\n",
    "            nome_social = fake.name_female() if \"Mulher\" in identidade_genero else fake.name_male()\n",
    "        hospede = (\n",
    "            hospede_id, nome_registro, nome_social, fake.cpf(),\n",
    "            fake.date_of_birth(minimum_age=18, maximum_age=85), sexo_nasc, identidade_genero,\n",
    "            random.choices(PRONOMES[\"opcoes\"], weights=PRONOMES[\"pesos\"], k=1)[0],\n",
    "            random.choices(ORIENTACOES_SEXUAIS[\"opcoes\"], weights=ORIENTACOES_SEXUAIS[\"pesos\"], k=1)[0],\n",
    "            'Brasil', random.choice(TIPOS_CLIENTE), f\"{nome_social.lower().replace(' ', '.')}@{random.choice(['gmail.com', 'hotmail.com'])}\",\n",
    "            fake.phone_number(), fake.state_abbr(), fake.city(), fake.postcode(),\n",
    "            data_ultima_estadia, ultimo_hotel_id\n",
    "        )\n",
    "        hospedes_completos.append(hospede)\n",
    "    return hospedes_completos\n",
    "\n",
    "schema_hospedes = StructType([\n",
    "    StructField(\"hospede_id\", IntegerType(), False), StructField(\"nome_registro\", StringType(), True),\n",
    "    StructField(\"nome_social\", StringType(), True), StructField(\"cpf\", StringType(), True),\n",
    "    StructField(\"data_nascimento\", DateType(), True), StructField(\"sexo_atribuido_nascimento\", StringType(), True),\n",
    "    StructField(\"identidade_genero\", StringType(), True), StructField(\"pronome\", StringType(), True),\n",
    "    StructField(\"orientacao_sexual\", StringType(), True), StructField(\"nacionalidade\", StringType(), True),\n",
    "    StructField(\"tipo_cliente\", StringType(), True), StructField(\"email\", StringType(), True),\n",
    "    StructField(\"telefone\", StringType(), True), StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"cidade\", StringType(), True), StructField(\"cep\", StringType(), True),\n",
    "    StructField(\"data_ultima_estadia\", DateType(), True), StructField(\"ultimo_hotel_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# --- EXECUÇÃO DO SCRIPT ---\n",
    "# Verifique se o nome da tabela de origem está correto\n",
    "TABLE_NAME = \"production.transient.pdv_consumos\" \n",
    "try:\n",
    "    df_consumos = spark.table(TABLE_NAME)\n",
    "    print(f\"Tabela '{TABLE_NAME}' carregada com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar a tabela de consumos: {e}\")\n",
    "    dbutils.notebook.exit(\"Tabela de consumos não encontrada.\")\n",
    "\n",
    "# Usando o alias F para garantir que as funções corretas do Spark sejam chamadas\n",
    "df_hospedes_a_criar = df_consumos.groupBy(\"hospede_id\") \\\n",
    "    .agg(\n",
    "        F.max(\"data_consumo\").alias(\"data_ultima_estadia\"),\n",
    "        F.first(\"hotel_id\", ignorenulls=True).alias(\"ultimo_hotel_id\")\n",
    "    )\n",
    "\n",
    "hospedes_info = df_hospedes_a_criar.collect()\n",
    "print(f\"Encontrados {len(hospedes_info)} hóspedes únicos para gerar perfis.\")\n",
    "\n",
    "print(\"Gerando perfis detalhados para cada hóspede...\")\n",
    "dados_hospedes = gerar_hospedes(hospedes_info)\n",
    "\n",
    "df_hospedes = spark.createDataFrame(dados_hospedes, schema_hospedes)\n",
    "\n",
    "print(\"\\nAmostra dos dados de hóspedes gerados:\")\n",
    "df_hospedes.select(\"hospede_id\", \"nome_social\", \"data_ultima_estadia\").show(5, truncate=False)\n",
    "\n",
    "print(\"\\nDistribuição de hóspedes por tipo de cliente:\")\n",
    "df_hospedes.groupBy(\"tipo_cliente\").count().orderBy(F.col(\"count\").desc()).show()\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "catalog_path = \"production.transient.crm_hospedes\"\n",
    "df_hospedes.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(catalog_path)\n",
    "print(f\"\\nTabela de hóspedes salva com sucesso em: {catalog_path}\")\n",
    "\n",
    "spark.sql(f\"OPTIMIZE {catalog_path} ZORDER BY (estado, tipo_cliente)\")\n",
    "print(\"Otimização concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e318cd17-1797-4966-8000-73da74d9598d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quartos"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DoubleType, BooleanType)\n",
    "\n",
    "# --- DOMÍNIOS DE DADOS ---\n",
    "\n",
    "# Definição dos tipos de quarto e suas características base\n",
    "TIPOS_QUARTO = {\n",
    "    'Standard': {\n",
    "        'capacidade': [1, 2], 'faixa_preco': [250.00, 400.00], \n",
    "        'descricao': 'Quarto confortável para estadias curtas.', 'percentual': 0.50\n",
    "    },\n",
    "    'Superior': {\n",
    "        'capacidade': [2, 3], 'faixa_preco': [400.00, 650.00], \n",
    "        'descricao': 'Quarto espaçoso com vista e amenities adicionais.', 'percentual': 0.25\n",
    "    },\n",
    "    'Suíte Junior': {\n",
    "        'capacidade': [2, 4], 'faixa_preco': [600.00, 900.00], \n",
    "        'descricao': 'Suíte com uma pequena área de estar integrada.', 'percentual': 0.15\n",
    "    },\n",
    "    'Suíte Executiva': {\n",
    "        'capacidade': [2, 4], 'faixa_preco': [800.00, 1300.00], \n",
    "        'descricao': 'Suíte com sala de estar separada e acesso ao lounge executivo.', 'percentual': 0.07\n",
    "    },\n",
    "    'Suíte Presidencial': {\n",
    "        'capacidade': [4, 6], 'faixa_preco': [1500.00, 3500.00], \n",
    "        'descricao': 'A acomodação mais luxuosa, com múltiplos ambientes.', 'percentual': 0.03\n",
    "    }\n",
    "}\n",
    "\n",
    "# Status possíveis para um quarto\n",
    "STATUS_QUARTO = ['Disponível', 'Ocupado', 'Em Limpeza', 'Em Manutenção', 'Bloqueado']\n",
    "\n",
    "# --- FUNÇÃO PRINCIPAL ---\n",
    "\n",
    "def gerar_quartos(hoteis_info, quartos_ocupados):\n",
    "    \"\"\"Gera a lista de todos os quartos para todos os hotéis.\"\"\"\n",
    "    quartos_gerados = []\n",
    "    quarto_id_global = 1001 # Inicia um ID único global para os quartos\n",
    "\n",
    "    for hotel in hoteis_info:\n",
    "        total_quartos_hotel = hotel['numero_quartos']\n",
    "        \n",
    "        # Cria uma lista com os tipos de quarto a serem gerados para este hotel\n",
    "        lista_tipos = []\n",
    "        for tipo, config in TIPOS_QUARTO.items():\n",
    "            num_quartos_tipo = int(total_quartos_hotel * config['percentual'])\n",
    "            lista_tipos.extend([tipo] * num_quartos_tipo)\n",
    "        \n",
    "        # Ajusta para garantir que o total de quartos seja exato\n",
    "        while len(lista_tipos) < total_quartos_hotel:\n",
    "            lista_tipos.append('Standard') # Adiciona Standard para completar\n",
    "        \n",
    "        random.shuffle(lista_tipos)\n",
    "\n",
    "        for i in range(total_quartos_hotel):\n",
    "            tipo_quarto_atual = lista_tipos[i]\n",
    "            config_quarto = TIPOS_QUARTO[tipo_quarto_atual]\n",
    "\n",
    "            # Define o número do quarto (ex: 101, 102, 201, 202)\n",
    "            andar = (i // 25) + 1 # Simula 25 quartos por andar\n",
    "            numero_no_andar = (i % 25) + 1\n",
    "            numero_quarto = f\"{andar:02d}{numero_no_andar:02d}\"\n",
    "\n",
    "            # Define o status do quarto\n",
    "            chave_ocupacao = (hotel['hotel_id'], int(numero_quarto))\n",
    "            if chave_ocupacao in quartos_ocupados:\n",
    "                status = random.choice(['Ocupado', 'Em Limpeza'])\n",
    "            else:\n",
    "                status = random.choices(\n",
    "                    ['Disponível', 'Em Manutenção', 'Bloqueado'], \n",
    "                    weights=[95, 4, 1]\n",
    "                )[0]\n",
    "            \n",
    "            # Calcula o preço base, influenciado pelas estrelas do hotel\n",
    "            preco_base = random.uniform(config_quarto['faixa_preco'][0], config_quarto['faixa_preco'][1])\n",
    "            preco_final = preco_base * (1 + (hotel['estrelas'] - 3) * 0.1) # +10% por estrela acima de 3\n",
    "\n",
    "            quarto = (\n",
    "                quarto_id_global,\n",
    "                hotel['hotel_id'],\n",
    "                int(numero_quarto),\n",
    "                tipo_quarto_atual,\n",
    "                config_quarto['descricao'],\n",
    "                random.randint(config_quarto['capacidade'][0], config_quarto['capacidade'][1]),\n",
    "                andar,\n",
    "                status,\n",
    "                round(preco_final, 2),\n",
    "                random.choice([True, False]) # Simula se o quarto é para fumantes\n",
    "            )\n",
    "            quartos_gerados.append(quarto)\n",
    "            quarto_id_global += 1\n",
    "            \n",
    "    return quartos_gerados\n",
    "\n",
    "# --- SCHEMA DO DATAFRAME ---\n",
    "schema_quartos = StructType([\n",
    "    StructField(\"quarto_id\", IntegerType(), False),\n",
    "    StructField(\"hotel_id\", IntegerType(), False),\n",
    "    StructField(\"numero_quarto\", IntegerType(), True),\n",
    "    StructField(\"tipo_quarto\", StringType(), True),\n",
    "    StructField(\"descricao\", StringType(), True),\n",
    "    StructField(\"capacidade_maxima\", IntegerType(), True),\n",
    "    StructField(\"andar\", IntegerType(), True),\n",
    "    StructField(\"status_atual\", StringType(), True),\n",
    "    StructField(\"preco_diaria_base\", DoubleType(), True),\n",
    "    StructField(\"permite_fumantes\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# --- EXECUÇÃO DO SCRIPT ---\n",
    "\n",
    "# 1. Carregar as tabelas de dependência\n",
    "try:\n",
    "    df_hoteis = spark.table(\"production.transient.pms_hoteis\")\n",
    "    df_consumos = spark.table(\"production.transient.pdv_consumos\")\n",
    "    print(\"Tabelas 'hoteis' e 'consumos' carregadas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar tabelas de dependência: {e}\")\n",
    "    dbutils.notebook.exit(\"Não foi possível carregar as tabelas base.\")\n",
    "\n",
    "# 2. Coletar informações dos hotéis para a geração\n",
    "hoteis_info = df_hoteis.select(\n",
    "    \"hotel_id\", \"numero_quartos\", \"estrelas\"\n",
    ").collect()\n",
    "\n",
    "# 3. Identificar quartos com atividade recente para definir o status como 'Ocupado'\n",
    "data_limite = datetime.now() - timedelta(days=2)\n",
    "df_quartos_ocupados = df_consumos.filter(col(\"data_consumo\") >= data_limite) \\\n",
    "    .select(\"hotel_id\", \"quarto_id\").distinct()\n",
    "\n",
    "# Cria um conjunto (set) para busca rápida e eficiente\n",
    "quartos_ocupados_set = set(\n",
    "    (row.hotel_id, row.quarto_id) for row in df_quartos_ocupados.collect()\n",
    ")\n",
    "print(f\"Encontrados {len(quartos_ocupados_set)} quartos com atividade recente.\")\n",
    "\n",
    "# 4. Gerar os dados dos quartos\n",
    "print(\"Gerando o inventário de quartos para cada hotel...\")\n",
    "dados_quartos = gerar_quartos(hoteis_info, quartos_ocupados_set)\n",
    "\n",
    "# 5. Criar o DataFrame Spark\n",
    "df_quartos = spark.createDataFrame(dados_quartos, schema_quartos)\n",
    "\n",
    "# 6. Análise e Verificação\n",
    "print(\"\\nAmostra dos quartos gerados:\")\n",
    "df_quartos.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nContagem de quartos por tipo e hotel (Top 10 combinações):\")\n",
    "df_quartos.groupBy(\"hotel_id\", \"tipo_quarto\").count().orderBy(\"count\", ascending=False).show(10)\n",
    "\n",
    "print(\"\\nDistribuição de status dos quartos:\")\n",
    "df_quartos.groupBy(\"status_atual\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "catalog_path = \"production.transient.pms_quartos\"\n",
    "\n",
    "df_quartos.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(catalog_path)\n",
    "\n",
    "print(f\"\\nTabela de quartos salva com sucesso em: {catalog_path}\")\n",
    "print(f\"Total de registros gravados: {df_quartos.count()}\")\n",
    "\n",
    "# Otimiza a tabela para melhorar o desempenho\n",
    "print(\"\\nAplicando OPTIMIZE na tabela...\")\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog_path}\n",
    "ZORDER BY (hotel_id, tipo_quarto, status_atual)\n",
    "\"\"\")\n",
    "print(\"Otimização concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1278d3-8027-4dbd-a4b3-16b769139e54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reservas Canal"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "# Importações de funções ajustadas para incluir as de agregação\n",
    "from pyspark.sql.functions import col, min as spark_min, first, count, avg \n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DoubleType, DateType)\n",
    "\n",
    "# --- DOMÍNIOS DE DADOS ---\n",
    "\n",
    "# OTAs (Online Travel Agencies) com suas comissões e prefixos de ID\n",
    "OTAS = [\n",
    "    {'nome': 'Booking.com', 'comissao_min': 12.0, 'comissao_max': 18.0, 'prefixo': 'BK'},\n",
    "    {'nome': 'Expedia', 'comissao_min': 15.0, 'comissao_max': 20.0, 'prefixo': 'EX'},\n",
    "    {'nome': 'Decolar', 'comissao_min': 12.0, 'comissao_max': 16.0, 'prefixo': 'DE'},\n",
    "    {'nome': 'Hoteis.com', 'comissao_min': 14.0, 'comissao_max': 19.0, 'prefixo': 'HO'},\n",
    "    {'nome': 'CVC', 'comissao_min': 8.0, 'comissao_max': 12.0, 'prefixo': 'CV'}\n",
    "]\n",
    "\n",
    "# Canais de reserva direta (sem comissão)\n",
    "CANAIS_DIRETOS = ['Website Hotel', 'Telefone', 'Balcão', 'E-mail Direto']\n",
    "\n",
    "# --- FUNÇÕES AUXILIARES ---\n",
    "\n",
    "def calcular_ajuste_comissao(tipo_quarto):\n",
    "    \"\"\"Retorna um percentual de ajuste de comissão baseado no luxo do quarto.\"\"\"\n",
    "    ajustes = {\n",
    "        'Standard': 0.0,\n",
    "        'Superior': 0.5,\n",
    "        'Suíte Junior': 1.5,\n",
    "        'Suíte Executiva': 2.5,\n",
    "        'Suíte Presidencial': 4.0\n",
    "    }\n",
    "    # Retorna 0.0 se o tipo de quarto for None ou não estiver no dicionário\n",
    "    return ajustes.get(tipo_quarto, 0.0) if tipo_quarto else 0.0\n",
    "\n",
    "# --- FUNÇÃO PRINCIPAL ---\n",
    "\n",
    "def gerar_canais_de_reserva(reservas_info):\n",
    "    \"\"\"\n",
    "    Gera os dados do canal de origem para uma lista de reservas existentes.\n",
    "    \"\"\"\n",
    "    dados_canal = []\n",
    "    \n",
    "    lista_fontes = ['OTA', 'Direto']\n",
    "    pesos_fontes = [75, 25]\n",
    "\n",
    "    for reserva in reservas_info:\n",
    "        reserva_id = reserva['reserva_id']\n",
    "        tipo_quarto = reserva['tipo_quarto']\n",
    "        data_checkin = reserva['data_primeiro_consumo']\n",
    "\n",
    "        fonte_reserva = random.choices(lista_fontes, weights=pesos_fontes)[0]\n",
    "        \n",
    "        data_reserva = data_checkin - timedelta(days=random.randint(1, 90))\n",
    "\n",
    "        if fonte_reserva == 'OTA':\n",
    "            ota = random.choice(OTAS)\n",
    "            canal = ota['nome']\n",
    "            reserva_canal_id = f\"{ota['prefixo']}-{reserva_id}\"\n",
    "            \n",
    "            comissao_base = random.uniform(ota['comissao_min'], ota['comissao_max'])\n",
    "            ajuste_quarto = calcular_ajuste_comissao(tipo_quarto)\n",
    "            comissao_final = round(comissao_base + ajuste_quarto, 2)\n",
    "            \n",
    "        else: # Canal Direto\n",
    "            canal = random.choice(CANAIS_DIRETOS)\n",
    "            reserva_canal_id = f\"DIR-{reserva_id}\"\n",
    "            comissao_final = 0.0\n",
    "\n",
    "        registro_canal = (\n",
    "            reserva_canal_id,\n",
    "            reserva_id,\n",
    "            canal,\n",
    "            fonte_reserva,\n",
    "            comissao_final,\n",
    "            data_reserva\n",
    "        )\n",
    "        dados_canal.append(registro_canal)\n",
    "        \n",
    "    return dados_canal\n",
    "\n",
    "# --- SCHEMA DO DATAFRAME ---\n",
    "schema_reservas_canal = StructType([\n",
    "    StructField(\"reserva_canal_id\", StringType(), False),\n",
    "    StructField(\"reserva_id\", IntegerType(), False),\n",
    "    StructField(\"canal_reserva\", StringType(), True),\n",
    "    StructField(\"fonte_reserva\", StringType(), True), \n",
    "    StructField(\"percentual_comissao\", DoubleType(), True),\n",
    "    StructField(\"data_reserva\", DateType(), True)\n",
    "])\n",
    "\n",
    "# --- EXECUÇÃO DO SCRIPT ---\n",
    "\n",
    "# 1. Carregar as tabelas de dependência\n",
    "try:\n",
    "    df_consumos = spark.table(\"production.transient.pdv_consumos\")\n",
    "    df_quartos = spark.table(\"production.transient.pms_quartos\")\n",
    "    print(\"Tabelas 'consumos' e 'quartos' carregadas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar tabelas de dependência: {e}\")\n",
    "    dbutils.notebook.exit(\"Não foi possível carregar as tabelas base.\")\n",
    "\n",
    "# 2. Identificar cada reserva única e buscar as informações necessárias\n",
    "df_reservas_base = df_consumos.groupBy(\"reserva_id\") \\\n",
    "    .agg(\n",
    "        first(\"hotel_id\", ignorenulls=True).alias(\"hotel_id\"),\n",
    "        first(\"quarto_id\", ignorenulls=True).alias(\"quarto_id\"),\n",
    "        spark_min(\"data_consumo\").alias(\"data_primeiro_consumo\")\n",
    "    )\n",
    "\n",
    "# 3. Juntar com a tabela de quartos para obter o tipo de quarto\n",
    "df_reservas_com_quarto = df_reservas_base.join(\n",
    "    df_quartos.select(\"hotel_id\", col(\"numero_quarto\").alias(\"quarto_id\"), \"tipo_quarto\"),\n",
    "    on=[\"hotel_id\", \"quarto_id\"],\n",
    "    how=\"left\"\n",
    ").select(\"reserva_id\", \"tipo_quarto\", \"data_primeiro_consumo\") # Seleciona apenas as colunas necessárias\n",
    "\n",
    "reservas_info = df_reservas_com_quarto.collect()\n",
    "print(f\"Encontradas {len(reservas_info)} reservas únicas para processar.\")\n",
    "\n",
    "# 4. Gerar os dados do canal para cada reserva\n",
    "print(\"Gerando dados do canal de origem para cada reserva...\")\n",
    "dados_finais = gerar_canais_de_reserva(reservas_info)\n",
    "\n",
    "# 5. Criar o DataFrame Spark\n",
    "df_reservas_canal = spark.createDataFrame(dados_finais, schema_reservas_canal)\n",
    "\n",
    "# 6. Análise e Verificação\n",
    "print(\"\\nAmostra dos dados gerados:\")\n",
    "df_reservas_canal.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nDistribuição de reservas por fonte:\")\n",
    "df_reservas_canal.groupBy(\"fonte_reserva\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# =======================================================================\n",
    "# CORREÇÃO APLICADA AQUI\n",
    "# =======================================================================\n",
    "print(\"\\nComissão média e total de reservas por canal:\")\n",
    "df_reservas_canal.groupBy(\"canal_reserva\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_reservas\"),\n",
    "        avg(\"percentual_comissao\").alias(\"comissao_media\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_reservas\").desc()) \\\n",
    "    .show()\n",
    "# =======================================================================\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "catalog_path = \"production.transient.crs_reservas_canal\"\n",
    "\n",
    "df_reservas_canal.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(catalog_path)\n",
    "\n",
    "print(f\"\\nTabela de canais de reserva salva com sucesso em: {catalog_path}\")\n",
    "print(f\"Total de registros gravados: {df_reservas_canal.count()}\")\n",
    "\n",
    "# Otimiza a tabela para melhorar o desempenho\n",
    "print(\"\\nAplicando OPTIMIZE na tabela...\")\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog_path}\n",
    "ZORDER BY (data_reserva, fonte_reserva)\n",
    "\"\"\")\n",
    "print(\"Otimização concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c78df9-5b09-4b8f-8681-5eafcf88c62f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reservas"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, min as spark_min, max as spark_max, first, \n",
    "                                   datediff, when, lit, rand)\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType,\n",
    "                               DoubleType, DateType)\n",
    "\n",
    "# --- DOMÍNIOS DE DADOS ---\n",
    "\n",
    "# Lista de observações comuns para adicionar variedade aos dados\n",
    "OBSERVACOES_COMUNS = [\n",
    "    'Solicitou andar alto', 'Preferência por quarto silencioso', 'Cliente VIP', \n",
    "    'Comemoração de aniversário', 'Lua de mel', 'Check-in antecipado solicitado',\n",
    "    'Check-out tardio aprovado', 'Necessita de berço', 'Alérgico a penas',\n",
    "    'Pagamento será feito no check-in', None # Para simular reservas sem observações\n",
    "]\n",
    "\n",
    "# --- FUNÇÃO PRINCIPAL ---\n",
    "\n",
    "def criar_reservas_consolidadas(spark):\n",
    "    \"\"\"\n",
    "    Consolida dados de várias tabelas para criar a tabela mestre de reservas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Carregar todas as tabelas de dependência\n",
    "    try:\n",
    "        df_consumos = spark.table(\"production.transient.pdv_consumos\")\n",
    "        df_quartos = spark.table(\"production.transient.pms_quartos\")\n",
    "        df_canais = spark.table(\"production.transient.crs_reservas_canal\")\n",
    "        df_hospedes = spark.table(\"production.transient.crm_hospedes\")\n",
    "        print(\"Tabelas de dependência carregadas com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar tabelas de dependência: {e}\")\n",
    "        dbutils.notebook.exit(\"Não foi possível carregar as tabelas base.\")\n",
    "\n",
    "    # 2. Agregar os consumos para definir o período da estadia de cada reserva\n",
    "    df_estadias = df_consumos.groupBy(\"reserva_id\") \\\n",
    "        .agg(\n",
    "            first(\"hospede_id\").alias(\"hospede_id\"),\n",
    "            first(\"hotel_id\").alias(\"hotel_id\"),\n",
    "            first(\"quarto_id\").alias(\"numero_quarto\"),\n",
    "            spark_min(\"data_consumo\").alias(\"data_checkin\"),\n",
    "            spark_max(\"data_consumo\").alias(\"data_checkout\")\n",
    "        )\n",
    "\n",
    "    # 3. Juntar as informações para enriquecer os dados da reserva\n",
    "    \n",
    "    # Adiciona dados do canal de origem (data da reserva, nome do canal)\n",
    "    df_com_canal = df_estadias.join(\n",
    "        df_canais.select(\"reserva_id\", \"data_reserva\", \"canal_reserva\"),\n",
    "        \"reserva_id\",\n",
    "        \"inner\" # Usamos inner join para garantir que toda reserva tenha um canal\n",
    "    )\n",
    "\n",
    "    # Adiciona dados do quarto (preço da diária)\n",
    "    df_com_quarto = df_com_canal.join(\n",
    "        df_quartos.select(\"hotel_id\", \"numero_quarto\", \"preco_diaria_base\"),\n",
    "        [\"hotel_id\", \"numero_quarto\"],\n",
    "        \"left\" # Usamos left join caso algum quarto não seja encontrado\n",
    "    )\n",
    "\n",
    "    # 4. Calcular campos derivados (noites, valor, status)\n",
    "    DATA_ATUAL = datetime.strptime(\"2025-10-04\", \"%Y-%m-%d\").date()\n",
    "\n",
    "    df_final = df_com_quarto \\\n",
    "        .withColumn(\"numero_noites\", datediff(col(\"data_checkout\"), col(\"data_checkin\")) + 1) \\\n",
    "        .withColumn(\"valor_total_estadia\", \n",
    "                    (col(\"numero_noites\") * col(\"preco_diaria_base\")).cast(DoubleType())) \\\n",
    "        .withColumn(\"status_reserva\", \n",
    "            when(col(\"data_checkout\") < DATA_ATUAL, \"Finalizada\")\n",
    "            .when((col(\"data_checkin\") <= DATA_ATUAL) & (col(\"data_checkout\") >= DATA_ATUAL), \"Hospedado\")\n",
    "            .otherwise(\"Confirmada\")\n",
    "        )\n",
    "\n",
    "    # 5. Adicionar observações aleatórias\n",
    "    # Gerar a lista de observações como uma coluna literal do Spark\n",
    "    observacoes_list = spark.createDataFrame([(o,) for o in OBSERVACOES_COMUNS], [\"observacoes\"])\n",
    "    # Adicionar um ID aleatório para fazer o join\n",
    "    df_final = df_final.withColumn(\"rand_join_key\", (rand() * len(OBSERVACOES_COMUNS)).cast(\"int\"))\n",
    "    observacoes_list = observacoes_list.withColumn(\"rand_join_key\", (rand() * len(OBSERVACOES_COMUNS)).cast(\"int\"))\n",
    "    \n",
    "    # Selecionar e renomear as colunas para o formato final\n",
    "    df_reservas = df_final.select(\n",
    "        col(\"reserva_id\"),\n",
    "        col(\"hospede_id\"),\n",
    "        col(\"numero_quarto\").alias(\"quarto_id\"),\n",
    "        col(\"hotel_id\"),\n",
    "        col(\"data_reserva\"),\n",
    "        col(\"data_checkin\"),\n",
    "        col(\"data_checkout\"),\n",
    "        col(\"numero_noites\"),\n",
    "        col(\"canal_reserva\"),\n",
    "        col(\"status_reserva\").alias(\"status\"),\n",
    "        col(\"valor_total_estadia\").alias(\"valor_total\"),\n",
    "        lit(None).cast(StringType()).alias(\"observacoes\") # Adicionado como placeholder\n",
    "    )\n",
    "    \n",
    "    return df_reservas\n",
    "\n",
    "# --- Schema Final ---\n",
    "# O schema é inferido diretamente da transformação do Spark\n",
    "    \n",
    "# --- EXECUÇÃO DO SCRIPT ---\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GeracaoReservas\").getOrCreate()\n",
    "\n",
    "print(\"Iniciando a consolidação da tabela de reservas...\")\n",
    "df_reservas = criar_reservas_consolidadas(spark)\n",
    "\n",
    "# --- Análise e Verificação ---\n",
    "print(f\"\\nTotal de reservas consolidadas: {df_reservas.count()}\")\n",
    "print(\"\\nAmostra dos dados finais:\")\n",
    "df_reservas.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nDistribuição de reservas por status:\")\n",
    "df_reservas.groupBy(\"status\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\nValor total de reservas por canal:\")\n",
    "df_reservas.groupBy(\"canal_reserva\") \\\n",
    "    .agg({\"valor_total\": \"sum\", \"reserva_id\": \"count\"}) \\\n",
    "    .withColumnRenamed(\"sum(valor_total)\", \"faturamento_total\") \\\n",
    "    .withColumnRenamed(\"count(reserva_id)\", \"numero_de_reservas\") \\\n",
    "    .orderBy(\"faturamento_total\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "# --- Salvando os Dados ---\n",
    "catalog_path = \"production.transient.pms_reservas\"\n",
    "\n",
    "df_reservas.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(catalog_path)\n",
    "\n",
    "print(f\"\\nTabela de reservas salva com sucesso em: {catalog_path}\")\n",
    "\n",
    "# Otimiza a tabela para melhorar o desempenho\n",
    "print(\"\\nAplicando OPTIMIZE na tabela...\")\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog_path}\n",
    "ZORDER BY (data_checkin, hotel_id)\n",
    "\"\"\")\n",
    "print(\"Otimização concluída com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CRM simulator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
