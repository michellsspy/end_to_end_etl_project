{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a1e9c-685a-4e67-a0c7-b144f78f67b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bce8330-5bc1-48ef-87a7-65b65984c517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "# Nome da tabela de destino no Catálogo Unity\n",
    "catalog_table = \"production.refined.d_localizacao\"\n",
    "\n",
    "# Verifica se a tabela já existe antes de tentar criá-la\n",
    "if not spark.catalog.tableExists(catalog_table):\n",
    "    print(f\"A tabela {catalog_table} não existe. Criando a estrutura...\")\n",
    "    \n",
    "    # Define o schema para a dimensão de hotéis, seguindo o padrão pk/sk\n",
    "    schema = T.StructType([\n",
    "        # Chave primária (hash da chave de negócio, ex: hotel_id)\n",
    "        T.StructField(\"pk_localizacao\", T.StringType(), False), \n",
    "        # Chave substituta (surrogate key) sequencial e única\n",
    "        T.StructField(\"sk_localizacao\", T.LongType(), False),\n",
    "        \n",
    "        # Atributos descritivos do hotel\n",
    "        T.StructField(\"pais\", T.StringType(), True),\n",
    "        T.StructField(\"estado\", T.StringType(), True),\n",
    "        T.StructField(\"cidade\", T.StringType(), True),\n",
    "        T.StructField(\"longitude\", T.StringType(), True),\n",
    "        T.StructField(\"latitude\", T.StringType(), True),\n",
    "\n",
    "        # Metadados de controle (SCD - Slowly Changing Dimensions)\n",
    "        T.StructField(\"start_date\", T.DateType(), True),\n",
    "        T.StructField(\"update_date\", T.DateType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Cria um DataFrame vazio com o schema definido\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Cria a tabela Delta gerenciada no catálogo\n",
    "    (\n",
    "        df_empty.write\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(catalog_table)\n",
    "    )\n",
    "\n",
    "    print(f\"Tabela Delta '{catalog_table}' criada com sucesso.\")\n",
    "else:\n",
    "    print(f\"A tabela '{catalog_table}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec5214c-16f0-469e-ba66-3ed405bf5d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Parâmetros ---\n",
    "catalog_table = \"production.refined.d_localizacao\"\n",
    "source_table = \"production.trusted.tb_hoteis\" \n",
    "\n",
    "print(\"Iniciando processo de merge para a tabela:\", catalog_table)\n",
    "\n",
    "# --- 1. Carregar Tabela de Destino ---\n",
    "try:\n",
    "    delta_table = DeltaTable.forName(spark, catalog_table)\n",
    "    df_dim_existente = delta_table.toDF()\n",
    "    is_initial_load = df_dim_existente.count() == 0\n",
    "except Exception as e:\n",
    "    # Captura exceção específica para tabela não encontrada\n",
    "    if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "        is_initial_load = True\n",
    "        print(f\"Tabela {catalog_table} não encontrada. Assumindo carga inicial.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# --- 2. Carregar e Preparar Dados de Origem ---\n",
    "# Seleciona as colunas da origem que correspondem à dimensão de destino.\n",
    "df_source = (\n",
    "    spark.read.table(source_table)\n",
    "    .select(\n",
    "        \"hotel_id\", \n",
    "        \"pais\",\n",
    "        \"estado\",\n",
    "        \"cidade\",\n",
    "        \"longitude\",\n",
    "        \"latitude\"\n",
    "    )\n",
    "    .dropDuplicates([\"hotel_id\"])\n",
    "    .withColumn(\n",
    "        \"pk_localizacao\",\n",
    "        F.sha2(F.col(\"hotel_id\").cast(T.StringType()), 256)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3. Identificar e Preparar Apenas os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    df_novos_hoteis = df_source\n",
    "else:\n",
    "    # Isola apenas os hotéis que são realmente novos (não existem no destino)\n",
    "    df_novos_hoteis = df_source.join(\n",
    "        df_dim_existente.select(\"pk_localizacao\"),\n",
    "        on=\"pk_localizacao\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "# Se não houver novos hotéis, encerra a execução para economizar recursos.\n",
    "if not is_initial_load and df_novos_hoteis.count() == 0:\n",
    "    print(\"Nenhum novo hotel para adicionar. Processo concluído.\")\n",
    "    # dbutils.notebook.exit(\"Nenhum novo hotel para adicionar.\") # Descomente para parar o notebook\n",
    "\n",
    "# --- 4. Gerar Chaves Surrogadas (SKs) Apenas para os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    last_id = 0\n",
    "else:\n",
    "    last_id_row = df_dim_existente.agg(F.max(\"sk_localizacao\")).collect()\n",
    "    last_id = last_id_row[0][0] if last_id_row and last_id_row[0][0] is not None else 0\n",
    "\n",
    "window = Window.orderBy(\"pk_localizacao\")\n",
    "df_com_novas_sks = (\n",
    "    df_novos_hoteis\n",
    "    .withColumn(\"sk_localizacao\", (F.row_number().over(window) + last_id).cast(T.LongType()))\n",
    "    .withColumn(\"start_date\", F.current_date())\n",
    "    .withColumn(\"update_date\", F.lit(None).cast(T.DateType()))\n",
    ")\n",
    "\n",
    "# --- 5. Criar o DataFrame Final para o MERGE ---\n",
    "# Une a fonte completa com as novas SKs (que só existem para os novos registros)\n",
    "df_final_source = df_source.join(\n",
    "    df_com_novas_sks.select(\"pk_localizacao\", \"sk_localizacao\", \"start_date\", \"update_date\"),\n",
    "    \"pk_localizacao\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# --- 6. Executar a Operação de MERGE ---\n",
    "# Condição para atualizar: apenas se dados descritivos do hotel mudarem.\n",
    "update_condition = \"\"\"\n",
    "    target.pais <> source.pais OR\n",
    "    target.estado <> source.estado OR\n",
    "    target.cidade <> source.cidade OR\n",
    "    target.longitude <> source.longitude OR\n",
    "    target.latitude <> source.latitude\n",
    "\"\"\"\n",
    "\n",
    "(\n",
    "    delta_table.alias(\"target\")\n",
    "    .merge(\n",
    "        df_final_source.alias(\"source\"),\n",
    "        \"target.pk_localizacao = source.pk_localizacao\"\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        condition=update_condition,\n",
    "        set={\n",
    "            \"pais\": F.col(\"source.pais\"),\n",
    "            \"estado\": F.col(\"source.estado\"),\n",
    "            \"cidade\": F.col(\"source.cidade\"),\n",
    "            \"longitude\": F.col(\"source.longitude\"),\n",
    "            \"latitude\": F.col(\"source.latitude\"),\n",
    "            \"update_date\": F.current_date()\n",
    "        }\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"pk_localizacao\": F.col(\"source.pk_localizacao\"),\n",
    "            \"sk_localizacao\": F.col(\"source.sk_localizacao\"),\n",
    "            \"pais\": F.col(\"source.pais\"),\n",
    "            \"estado\": F.col(\"source.estado\"),\n",
    "            \"cidade\": F.col(\"source.cidade\"),\n",
    "            \"longitude\": F.col(\"source.longitude\"),\n",
    "            \"latitude\": F.col(\"source.latitude\"),\n",
    "            \"start_date\": F.col(\"source.start_date\"),\n",
    "            \"update_date\": F.col(\"source.update_date\")\n",
    "        }\n",
    "    )\n",
    ").execute()\n",
    "\n",
    "print(f\"Merge/upsert concluído com sucesso na tabela: {catalog_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2160336401238899,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "d_localizacao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
