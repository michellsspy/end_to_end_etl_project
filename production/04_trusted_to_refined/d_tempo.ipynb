{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5576b4-9909-4354-840a-60ae345cf788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01af831e-0f50-4141-8a5c-b3c291061239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# PASSO 1: CONFIGURAÇÃO E PARÂMETROS\n",
    "# =======================================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType\n",
    "import holidays\n",
    "\n",
    "# -- Parâmetros Configuráveis --\n",
    "data_inicio = \"2015-01-01\"\n",
    "data_fim = \"2035-12-31\"\n",
    "nome_tabela_final = \"production.refined.dim_tempo\"\n",
    "\n",
    "print(f\"Gerando dimensão de tempo de {data_inicio} a {data_fim}.\")\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 2: GERAR A SEQUÊNCIA DE DATAS BASE\n",
    "# =======================================================================\n",
    "# A função 'sequence' é a forma mais performática de gerar um intervalo de datas no Spark.\n",
    "df_base_dates = spark.sql(f\"SELECT explode(sequence(to_date('{data_inicio}'), to_date('{data_fim}'), interval 1 day)) AS data_completa\")\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 3: FUNÇÃO UDF PARA IDENTIFICAR FERIADOS\n",
    "# =======================================================================\n",
    "\n",
    "def identificar_feriado(data):\n",
    "    \"\"\"\n",
    "    Usa a biblioteca 'holidays' para verificar se uma data é feriado nacional no Brasil.\n",
    "    Retorna o nome do feriado ou 'Não é Feriado'.\n",
    "    \"\"\"\n",
    "    feriados_br = holidays.country_holidays('BR')\n",
    "    return feriados_br.get(data, \"Não é Feriado\")\n",
    "\n",
    "# Registra a função como uma UDF (User-Defined Function)\n",
    "feriado_udf = F.udf(identificar_feriado, StringType())\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 4: ENRIQUECER O DATAFRAME COM TODOS OS ATRIBUTOS E MÉTRICAS\n",
    "# =======================================================================\n",
    "print(\"Calculando atributos da dimensão de tempo...\")\n",
    "\n",
    "df_dim_tempo = (\n",
    "    df_base_dates\n",
    "    # --- Chave Primária ---\n",
    "    .withColumn(\"id_tempo\", F.date_format(F.col(\"data_completa\"), \"yyyyMMdd\").cast(IntegerType()))\n",
    "\n",
    "    # --- Componentes de Ano ---\n",
    "    .withColumn(\"ano\", F.year(F.col(\"data_completa\")))\n",
    "\n",
    "    # --- Componentes de Semestre ---\n",
    "    .withColumn(\"semestre\", F.when(F.quarter(F.col(\"data_completa\")).isin([1, 2]), 1).otherwise(2))\n",
    "    .withColumn(\"nome_semestre\", F.when(F.col(\"semestre\") == 1, \"1º Semestre\").otherwise(\"2º Semestre\"))\n",
    "\n",
    "    # --- Componentes de Trimestre ---\n",
    "    .withColumn(\"trimestre\", F.quarter(F.col(\"data_completa\")))\n",
    "    .withColumn(\"nome_trimestre\", F.concat(F.col(\"trimestre\").cast(StringType()), F.lit(\"º Trimestre\")))\n",
    "\n",
    "    # --- Componentes de Mês ---\n",
    "    .withColumn(\"mes\", F.month(F.col(\"data_completa\")))\n",
    "    .withColumn(\"nome_mes\", F.date_format(F.col(\"data_completa\"), \"MMMM\")) # 'MMMM' retorna o nome por extenso\n",
    "    .withColumn(\"nome_mes_abrev\", F.date_format(F.col(\"data_completa\"), \"MMM\")) # 'MMM' retorna a abreviação\n",
    "\n",
    "    # --- Componentes de Semana ---\n",
    "    .withColumn(\"semana_do_ano\", F.weekofyear(F.col(\"data_completa\")))\n",
    "    .withColumn(\"dia_da_semana\", F.dayofweek(F.col(\"data_completa\"))) # Domingo=1, Sábado=7\n",
    "    .withColumn(\"nome_dia_da_semana\", F.date_format(F.col(\"data_completa\"), \"EEEE\")) # 'EEEE' retorna o nome por extenso\n",
    "\n",
    "    # --- Componentes de Dia ---\n",
    "    .withColumn(\"dia_do_ano\", F.dayofyear(F.col(\"data_completa\")))\n",
    "    .withColumn(\"dia_do_mes\", F.dayofmonth(F.col(\"data_completa\")))\n",
    "    \n",
    "    # --- Feriados (usando a UDF) ---\n",
    "    .withColumn(\"nome_feriado\", feriado_udf(F.col(\"data_completa\")))\n",
    "\n",
    "    # --- Flags (Sinalizadores) ---\n",
    "    .withColumn(\"flag_fim_de_semana\", F.when(F.col(\"dia_da_semana\").isin([1, 7]), 1).otherwise(0).cast(IntegerType()))\n",
    "    .withColumn(\"flag_feriado\", F.when(F.col(\"nome_feriado\") != \"Não é Feriado\", 1).otherwise(0).cast(IntegerType()))\n",
    "    .withColumn(\"flag_ultimo_dia_mes\", F.when(F.col(\"data_completa\") == F.last_day(F.col(\"data_completa\")), 1).otherwise(0).cast(IntegerType()))\n",
    ")\n",
    "\n",
    "# --- Reordenando as colunas para uma melhor organização ---\n",
    "ordem_colunas = [\n",
    "    \"id_tempo\", \"data_completa\", \"ano\", \"semestre\", \"nome_semestre\", \"trimestre\", \"nome_trimestre\",\n",
    "    \"mes\", \"nome_mes\", \"nome_mes_abrev\", \"semana_do_ano\", \"dia_da_semana\", \"nome_dia_da_semana\",\n",
    "    \"dia_do_ano\", \"dia_do_mes\", \"nome_feriado\", \"flag_fim_de_semana\", \"flag_feriado\",\n",
    "    \"flag_ultimo_dia_mes\"\n",
    "]\n",
    "df_dim_tempo_final = df_dim_tempo.select(ordem_colunas)\n",
    "\n",
    "print(\"Cálculos concluídos. Amostra da dimensão:\")\n",
    "display(df_dim_tempo_final.limit(10))\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# PASSO 5: SALVAR A TABELA DELTA FINAL\n",
    "# =======================================================================\n",
    "print(f\"\\nSalvando tabela final em: {nome_tabela_final}\")\n",
    "\n",
    "(\n",
    "    df_dim_tempo_final.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(nome_tabela_final)\n",
    ")\n",
    "\n",
    "print(\"Tabela 'dim_tempo' salva com sucesso!\")\n",
    "# Otimização final\n",
    "spark.sql(f\"OPTIMIZE {nome_tabela_final} ZORDER BY (data_completa)\")\n",
    "print(\"Otimização concluída.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5150682532233932,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "d_tempo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
