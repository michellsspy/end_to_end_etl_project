{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a1e9c-685a-4e67-a0c7-b144f78f67b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bce8330-5bc1-48ef-87a7-65b65984c517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "catalog_table = \"production.refined.d_hospedes\"\n",
    "\n",
    "if not spark.catalog.tableExists(catalog_table):\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"pk_hospede\", T.StringType(), True),\n",
    "        T.StructField(\"sk_hospede\", T.LongType(), True),\n",
    "        T.StructField(\"nome\", T.StringType(), True),\n",
    "        T.StructField(\"data_nascimento\", T.DateType(), True),\n",
    "        T.StructField(\"sexo\", T.StringType(), True),\n",
    "        T.StructField(\"nacionalidade\", T.StringType(), True),\n",
    "        T.StructField(\"tipo_cliente\", T.StringType(), True),\n",
    "        T.StructField(\"email\", T.StringType(), True),\n",
    "        T.StructField(\"telefone\", T.StringType(), True),\n",
    "        T.StructField(\"idade\", T.IntegerType(), True),        \n",
    "        T.StructField(\"start_date\", T.DateType(), True),\n",
    "        T.StructField(\"update_date\", T.DateType(), True)\n",
    "    ])\n",
    "\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Cria tabela Delta no catálogo\n",
    "    df_empty.write.format(\"delta\").saveAsTable(catalog_table)\n",
    "\n",
    "    print(\"Tabela Delta criada com sucesso em:\", catalog_table)\n",
    "else:\n",
    "    print(\"Tabela já existe:\", catalog_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec5214c-16f0-469e-ba66-3ed405bf5d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Parâmetros ---\n",
    "catalog_table = \"production.refined.d_hospedes\"\n",
    "source_table = \"production.trusted.tb_hospedes\"\n",
    "\n",
    "print(\"Iniciando processo de merge para a tabela:\", catalog_table)\n",
    "\n",
    "# --- 1. Carregar Tabela de Destino ---\n",
    "try:\n",
    "    delta_table = DeltaTable.forName(spark, catalog_table)\n",
    "    df_dim_existente = delta_table.toDF()\n",
    "    is_initial_load = df_dim_existente.count() == 0\n",
    "except Exception:\n",
    "    is_initial_load = True\n",
    "    print(f\"Tabela {catalog_table} não encontrada. Assumindo carga inicial.\")\n",
    "\n",
    "# --- 2. Carregar e Preparar Dados de Origem ---\n",
    "# Lê todas as colunas necessárias da origem e calcula a chave primária (pk).\n",
    "df_source = (\n",
    "    spark.read.table(source_table)\n",
    "    .select(\n",
    "        \"hospede_id\", \"nome\", \"data_nascimento\", \"sexo\", \"nacionalidade\",\n",
    "        \"tipo_cliente\", \"email\", \"telefone\", \"idade\"\n",
    "    )\n",
    "    .dropDuplicates([\"hospede_id\"])\n",
    "    .withColumn(\n",
    "        \"pk_hospede\",\n",
    "        F.sha2(F.concat_ws(\"||\", F.col(\"hospede_id\")), 256)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3. Identificar e Preparar Apenas os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    df_novos_hospedes = df_source\n",
    "else:\n",
    "    # Isola apenas os registros que são realmente novos\n",
    "    df_novos_hospedes = df_source.join(\n",
    "        df_dim_existente.select(\"pk_hospede\"),\n",
    "        on=\"pk_hospede\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "# Calcula o último sk_hospede para continuar a sequência.\n",
    "if is_initial_load:\n",
    "    last_id = 0\n",
    "else:\n",
    "    last_id_row = df_dim_existente.agg(F.max(\"sk_hospede\")).collect()\n",
    "    last_id = last_id_row[0][0] if last_id_row and last_id_row[0][0] is not None else 0\n",
    "\n",
    "# Gera novas SKs e metadados somente para os novos registros.\n",
    "window = Window.orderBy(\"pk_hospede\")\n",
    "df_com_novas_sks = (\n",
    "    df_novos_hospedes.withColumn(\"sk_hospede\", (F.row_number().over(window) + last_id).cast(T.LongType()))\n",
    "    .withColumn(\"start_date\", F.current_date())\n",
    "    .withColumn(\"update_date\", F.lit(None).cast(T.DateType()))\n",
    ")\n",
    "\n",
    "# --- 4. Criar o DataFrame Final para o MERGE ---\n",
    "# Unimos o df_source (com todos os registros) com as novas SKs (apenas para novos registros).\n",
    "# O resultado é um DataFrame com todos os registros de origem, onde os novos têm sk_hospede\n",
    "# e os antigos têm sk_hospede nulo (o que não importa, pois eles irão para o UPDATE).\n",
    "df_final_source = df_source.join(\n",
    "    df_com_novas_sks.select(\"pk_hospede\", \"sk_hospede\", \"start_date\", \"update_date\"),\n",
    "    \"pk_hospede\",\n",
    "    \"left\"\n",
    ").sort(\"sk_hospede\")\n",
    "\n",
    "# --- 5. Executar a Operação de MERGE ---\n",
    "# Condição para atualizar: apenas se dados descritivos mudarem.\n",
    "update_condition = \"target.nome <> source.nome OR target.email <> source.email OR target.nacionalidade <> source.nacionalidade OR target.telefone <> source.telefone OR target.tipo_cliente <> source.tipo_cliente OR target.idade <> source.idade\"\n",
    "(\n",
    "    delta_table.alias(\"target\")\n",
    "    .merge(\n",
    "        df_final_source.alias(\"source\"),\n",
    "        \"target.pk_hospede = source.pk_hospede\"\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        condition=update_condition,\n",
    "        set={\n",
    "            \"nome\": F.col(\"source.nome\"), \n",
    "            \"data_nascimento\": F.col(\"source.data_nascimento\"),\n",
    "            \"sexo\": F.col(\"source.sexo\"), \n",
    "            \"nacionalidade\": F.col(\"source.nacionalidade\"),\n",
    "            \"tipo_cliente\": F.col(\"source.tipo_cliente\"), \n",
    "            \"email\": F.col(\"source.email\"),\n",
    "            \"telefone\": F.col(\"source.telefone\"), \n",
    "            \"idade\": F.col(\"source.idade\"),\n",
    "            \"update_date\": F.current_date()\n",
    "        }\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"pk_hospede\": F.col(\"source.pk_hospede\"),\n",
    "            \"sk_hospede\": F.col(\"source.sk_hospede\"), # Agora esta coluna existe no df_final_source\n",
    "            \"nome\": F.col(\"source.nome\"),\n",
    "            \"data_nascimento\": F.col(\"source.data_nascimento\"),\n",
    "            \"sexo\": F.col(\"source.sexo\"),\n",
    "            \"nacionalidade\": F.col(\"source.nacionalidade\"),\n",
    "            \"tipo_cliente\": F.col(\"source.tipo_cliente\"),\n",
    "            \"email\": F.col(\"source.email\"),\n",
    "            \"telefone\": F.col(\"source.telefone\"),\n",
    "            \"idade\": F.col(\"source.idade\"),\n",
    "            \"start_date\": F.col(\"source.start_date\"),\n",
    "            \"update_date\": F.col(\"source.update_date\")\n",
    "        }\n",
    "    )\n",
    ").execute()\n",
    "\n",
    "print(f\"Merge/upsert concluído com sucesso na tabela: {catalog_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8a3646-565c-4c50-adad-b3f85a6c94c6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759520564966}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select DISTINCT sexo from production.refined.d_hospedes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8401717396897864,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "d_hospedes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
