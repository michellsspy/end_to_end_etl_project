{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a1e9c-685a-4e67-a0c7-b144f78f67b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bce8330-5bc1-48ef-87a7-65b65984c517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "# Nome da tabela de destino no Catálogo Unity\n",
    "catalog_table = \"production.refined.d_quartos\"\n",
    "\n",
    "# Verifica se a tabela já existe antes de tentar criá-la\n",
    "if not spark.catalog.tableExists(catalog_table):\n",
    "    print(f\"A tabela {catalog_table} não existe. Criando a estrutura...\")\n",
    "    \n",
    "    # Define o schema para a dimensão de quartos\n",
    "    schema = T.StructType([\n",
    "        # Chave primária (hash da chave de negócio composta: hotel_id + quarto_id)\n",
    "        T.StructField(\"pk_quarto\", T.StringType(), False),\n",
    "        # Chave substituta (surrogate key) sequencial e única\n",
    "        T.StructField(\"sk_quarto\", T.LongType(), False),\n",
    "        \n",
    "        # Chaves de negócio originais para referência\n",
    "        T.StructField(\"hotel_id\", T.IntegerType(), True),\n",
    "        T.StructField(\"quarto_id\", T.IntegerType(), True),\n",
    "        \n",
    "        # Atributos descritivos do quarto, conforme solicitado\n",
    "        T.StructField(\"tipo_quarto\", T.StringType(), True),\n",
    "        T.StructField(\"capacidade_maxima\", T.IntegerType(), True),\n",
    "        # Usar DecimalType é a melhor prática para valores monetários\n",
    "        T.StructField(\"preco_diaria_base\", T.DecimalType(10, 2), True),\n",
    "\n",
    "        # Metadados de controle (SCD - Slowly Changing Dimensions)\n",
    "        T.StructField(\"start_date\", T.DateType(), True),\n",
    "        T.StructField(\"update_date\", T.DateType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Cria um DataFrame vazio com o schema definido\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Cria a tabela Delta gerenciada no catálogo\n",
    "    (\n",
    "        df_empty.write\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(catalog_table)\n",
    "    )\n",
    "\n",
    "    print(f\"Tabela Delta '{catalog_table}' criada com sucesso.\")\n",
    "else:\n",
    "    print(f\"A tabela '{catalog_table}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec5214c-16f0-469e-ba66-3ed405bf5d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Parâmetros ---\n",
    "catalog_table = \"production.refined.d_quartos\"\n",
    "source_table = \"production.trusted.tb_quartos\" \n",
    "\n",
    "print(\"Iniciando processo de merge para a tabela:\", catalog_table)\n",
    "\n",
    "# --- 1. Carregar Tabela de Destino ---\n",
    "try:\n",
    "    delta_table = DeltaTable.forName(spark, catalog_table)\n",
    "    df_dim_existente = delta_table.toDF()\n",
    "    is_initial_load = df_dim_existente.count() == 0\n",
    "except Exception as e:\n",
    "    if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "        is_initial_load = True\n",
    "        print(f\"Tabela {catalog_table} não encontrada. Assumindo carga inicial.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# --- 2. Carregar e Preparar Dados de Origem ---\n",
    "# Seleciona as colunas da origem que correspondem à dimensão de destino.\n",
    "df_source = (\n",
    "    spark.read.table(source_table)\n",
    "    .select(\n",
    "        \"hotel_id\", \n",
    "        \"quarto_id\",\n",
    "        \"tipo_quarto\",\n",
    "        \"capacidade_maxima\",\n",
    "        \"preco_diaria_base\"\n",
    "    )\n",
    "    .dropDuplicates([\"hotel_id\", \"quarto_id\"]) # A chave de negócio é composta\n",
    "    .withColumn(\n",
    "        \"pk_quarto\",\n",
    "        # A PK é um hash da combinação de hotel_id e quarto_id\n",
    "        F.sha2(F.concat_ws(\"||\", F.col(\"hotel_id\"), F.col(\"quarto_id\")), 256)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3. Identificar e Preparar Apenas os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    df_novos_quartos = df_source\n",
    "else:\n",
    "    # Isola apenas os quartos que são realmente novos (não existem no destino)\n",
    "    df_novos_quartos = df_source.join(\n",
    "        df_dim_existente.select(\"pk_quarto\"),\n",
    "        on=\"pk_quarto\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "if not is_initial_load and df_novos_quartos.count() == 0:\n",
    "    print(\"Nenhum novo quarto para adicionar. Processo concluído.\")\n",
    "    # dbutils.notebook.exit(\"Nenhum novo quarto para adicionar.\")\n",
    "\n",
    "# --- 4. Gerar Chaves Surrogadas (SKs) Apenas para os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    last_id = 0\n",
    "else:\n",
    "    last_id_row = df_dim_existente.agg(F.max(\"sk_quarto\")).collect()\n",
    "    last_id = last_id_row[0][0] if last_id_row and last_id_row[0][0] is not None else 0\n",
    "\n",
    "window = Window.orderBy(\"pk_quarto\")\n",
    "df_com_novas_sks = (\n",
    "    df_novos_quartos\n",
    "    .withColumn(\"sk_quarto\", (F.row_number().over(window) + last_id).cast(T.LongType()))\n",
    "    .withColumn(\"start_date\", F.current_date())\n",
    "    .withColumn(\"update_date\", F.lit(None).cast(T.DateType()))\n",
    ")\n",
    "\n",
    "# --- 5. Criar o DataFrame Final para o MERGE ---\n",
    "df_final_source = df_source.join(\n",
    "    df_com_novas_sks.select(\"pk_quarto\", \"sk_quarto\", \"start_date\", \"update_date\"),\n",
    "    \"pk_quarto\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# --- 6. Executar a Operação de MERGE ---\n",
    "# Condição para atualizar: se os atributos de um quarto existente mudarem.\n",
    "update_condition = \"\"\"\n",
    "    target.tipo_quarto <> source.tipo_quarto OR\n",
    "    target.capacidade_maxima <> source.capacidade_maxima OR\n",
    "    target.preco_diaria_base <> source.preco_diaria_base\n",
    "\"\"\"\n",
    "\n",
    "(\n",
    "    delta_table.alias(\"target\")\n",
    "    .merge(\n",
    "        df_final_source.alias(\"source\"),\n",
    "        \"target.pk_quarto = source.pk_quarto\"\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        condition=update_condition,\n",
    "        set={\n",
    "            \"tipo_quarto\": F.col(\"source.tipo_quarto\"),\n",
    "            \"capacidade_maxima\": F.col(\"source.capacidade_maxima\"),\n",
    "            \"preco_diaria_base\": F.col(\"source.preco_diaria_base\"),\n",
    "            \"update_date\": F.current_date()\n",
    "        }\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"pk_quarto\": F.col(\"source.pk_quarto\"),\n",
    "            \"sk_quarto\": F.col(\"source.sk_quarto\"),\n",
    "            \"hotel_id\": F.col(\"source.hotel_id\"),\n",
    "            \"quarto_id\": F.col(\"source.quarto_id\"),\n",
    "            \"tipo_quarto\": F.col(\"source.tipo_quarto\"),\n",
    "            \"capacidade_maxima\": F.col(\"source.capacidade_maxima\"),\n",
    "            \"preco_diaria_base\": F.col(\"source.preco_diaria_base\"),\n",
    "            \"start_date\": F.col(\"source.start_date\"),\n",
    "            \"update_date\": F.col(\"source.update_date\")\n",
    "        }\n",
    "    )\n",
    ").execute()\n",
    "\n",
    "print(f\"Merge/upsert concluído com sucesso na tabela: {catalog_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5150682532233932,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "d_quartos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
