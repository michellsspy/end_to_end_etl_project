{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a1e9c-685a-4e67-a0c7-b144f78f67b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bce8330-5bc1-48ef-87a7-65b65984c517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "# Nome da tabela de destino no Catálogo Unity\n",
    "catalog_table = \"production.refined.d_canal\"\n",
    "\n",
    "# Verifica se a tabela já existe antes de tentar criá-la\n",
    "if not spark.catalog.tableExists(catalog_table):\n",
    "    print(f\"A tabela {catalog_table} não existe. Criando a estrutura...\")\n",
    "    \n",
    "    # Define o schema para a dimensão de formas de pagamento\n",
    "    schema = T.StructType([\n",
    "        # Chave primária (hash da chave de negócio: canal)\n",
    "        T.StructField(\"pk_canal\", T.StringType(), False),\n",
    "        # Chave substituta (surrogate key) sequencial e única\n",
    "        T.StructField(\"sk_canal\", T.LongType(), False),\n",
    "        \n",
    "        # Atributos descritivos da forma de pagamento\n",
    "        T.StructField(\"canal_reserva\", T.StringType(), True),\n",
    "        T.StructField(\"fonte_reserva\", T.StringType(), True), # Ex: 'Crédito', 'Débito', 'Transferência'\n",
    "\n",
    "        # Metadados de controle (SCD - Slowly Changing Dimensions)\n",
    "        T.StructField(\"start_date\", T.DateType(), True),\n",
    "        T.StructField(\"update_date\", T.DateType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Cria um DataFrame vazio com o schema definido\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Cria a tabela Delta gerenciada no catálogo\n",
    "    (\n",
    "        df_empty.write\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(catalog_table)\n",
    "    )\n",
    "\n",
    "    print(f\"Tabela Delta '{catalog_table}' criada com sucesso.\")\n",
    "else:\n",
    "    print(f\"A tabela '{catalog_table}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec5214c-16f0-469e-ba66-3ed405bf5d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Parâmetros ---\n",
    "catalog_table = \"production.refined.d_canal\"\n",
    "# Assumindo que a origem das formas de pagamento também está na tabela de faturas\n",
    "source_table = \"production.trusted.tb_reservas\" \n",
    "\n",
    "print(\"Iniciando processo de merge para a tabela:\", catalog_table)\n",
    "\n",
    "# --- 1. Carregar Tabela de Destino ---\n",
    "try:\n",
    "    delta_table = DeltaTable.forName(spark, catalog_table)\n",
    "    df_dim_existente = delta_table.toDF()\n",
    "    is_initial_load = df_dim_existente.count() == 0\n",
    "except Exception as e:\n",
    "    if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "        is_initial_load = True\n",
    "        print(f\"Tabela {catalog_table} não encontrada. Assumindo carga inicial.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# --- 2. Carregar e Preparar Dados de Origem ---\n",
    "# Obtém as formas de pagamento únicas da origem e calcula a chave primária (pk).\n",
    "# ASSUNÇÃO: A tabela de origem possui as colunas 'canal_reserva' e 'fonte_reserva'.\n",
    "# Se os nomes forem diferentes, ajuste-os no .select() abaixo.\n",
    "df_source = (\n",
    "    spark.read.table(source_table)\n",
    "    .select(\"canal_reserva\", \"fonte_reserva\")\n",
    "    .filter(F.col(\"canal_reserva\").isNotNull())\n",
    "    .dropDuplicates([\"canal_reserva\"]) # A chave de negócio é o nome\n",
    "    .withColumn(\n",
    "        \"pk_canal\",\n",
    "        F.sha2(F.col(\"canal_reserva\"), 256)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3. Identificar Apenas os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    df_novas_formas_pagamento = df_source\n",
    "else:\n",
    "    # Usa um LEFT ANTI JOIN para isolar apenas os registros que NÃO existem no destino.\n",
    "    df_novas_formas_pagamento = df_source.join(\n",
    "        df_dim_existente.select(\"pk_canal\"),\n",
    "        on=\"pk_canal\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "if not is_initial_load and df_novas_formas_pagamento.count() == 0:\n",
    "    print(\"Nenhuma nova forma de pagamento para adicionar. Processo concluído.\")\n",
    "    # dbutils.notebook.exit(\"Nenhuma nova forma de pagamento para adicionar.\")\n",
    "\n",
    "# --- 4. Gerar Chaves Surrogadas (SKs) Apenas para os Novos Registros ---\n",
    "if is_initial_load:\n",
    "    last_id = 0\n",
    "else:\n",
    "    last_id_row = df_dim_existente.agg(F.max(\"sk_canal\")).collect()\n",
    "    last_id = last_id_row[0][0] if last_id_row and last_id_row[0][0] is not None else 0\n",
    "\n",
    "window = Window.orderBy(\"pk_canal\")\n",
    "df_com_novas_sks = (\n",
    "    df_novas_formas_pagamento\n",
    "    .withColumn(\"sk_canal\", (F.row_number().over(window) + last_id).cast(T.LongType()))\n",
    "    .withColumn(\"start_date\", F.current_date())\n",
    "    .withColumn(\"update_date\", F.lit(None).cast(T.DateType()))\n",
    ")\n",
    "\n",
    "# --- 5. Executar a Operação de MERGE (Apenas com Inserção) ---\n",
    "# Seguindo o mesmo padrão de d_servicos, não há cláusula de UPDATE.\n",
    "(\n",
    "    delta_table.alias(\"target\")\n",
    "    .merge(\n",
    "        df_com_novas_sks.alias(\"source\"),\n",
    "        \"target.pk_canal = source.pk_canal\"\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"pk_canal\": F.col(\"source.pk_canal\"),\n",
    "            \"sk_canal\": F.col(\"source.sk_canal\"),\n",
    "            \"canal_reserva\": F.col(\"source.canal_reserva\"),\n",
    "            \"fonte_reserva\": F.col(\"source.fonte_reserva\"),\n",
    "            \"start_date\": F.col(\"source.start_date\"),\n",
    "            \"update_date\": F.col(\"source.update_date\")\n",
    "        }\n",
    "    )\n",
    ").execute()\n",
    "\n",
    "print(f\"Merge/upsert concluído com sucesso na tabela: {catalog_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2160336401238922,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "d_canal",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
