{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a1e9c-685a-4e67-a0c7-b144f78f67b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bce8330-5bc1-48ef-87a7-65b65984c517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "catalog_table = \"production.refined.d_servicos\"\n",
    "\n",
    "if not spark.catalog.tableExists(catalog_table):\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"pk_servico\", T.StringType(), True),\n",
    "        T.StructField(\"sk_servico\", T.LongType(), True),\n",
    "        T.StructField(\"tipo_servico\", T.StringType(), True),\n",
    "        T.StructField(\"descricao\", T.StringType(), True),\n",
    "        T.StructField(\"start_date\", T.DateType(), True),\n",
    "        T.StructField(\"update_date\", T.DateType(), True)\n",
    "    ])\n",
    "\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Cria tabela Delta no catálogo\n",
    "    df_empty.write.format(\"delta\").saveAsTable(catalog_table)\n",
    "\n",
    "    print(\"Tabela Delta criada com sucesso em:\", catalog_table)\n",
    "else:\n",
    "    print(\"Tabela já existe:\", catalog_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec5214c-16f0-469e-ba66-3ed405bf5d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Nome da tabela no catálogo ---\n",
    "catalog_table = \"production.refined.d_servicos\"\n",
    "\n",
    "# --- Cria tabela Delta vazia se não existir ---\n",
    "if not spark.catalog.tableExists(catalog_table):\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"pk_servico\", T.StringType(), True),\n",
    "        T.StructField(\"sk_servico\", T.LongType(), True),\n",
    "        T.StructField(\"tipo_servico\", T.StringType(), True),\n",
    "        T.StructField(\"descricao\", T.StringType(), True),\n",
    "        T.StructField(\"start_date\", T.DateType(), True),\n",
    "        T.StructField(\"update_date\", T.DateType(), True)\n",
    "    ])\n",
    "    df_empty = spark.createDataFrame([], schema)\n",
    "    df_empty.write.format(\"delta\").saveAsTable(catalog_table)\n",
    "    print(\"Tabela Delta criada com sucesso:\", catalog_table)\n",
    "else:\n",
    "    print(\"Tabela já existe:\", catalog_table)\n",
    "\n",
    "# --- Carrega a dimensão existente ---\n",
    "delta_table = DeltaTable.forName(spark, catalog_table)\n",
    "df_dim_existente = delta_table.toDF()\n",
    "\n",
    "# --- Consome os dados de entrada ---\n",
    "df_consumos = (\n",
    "    spark.read.table(\"production.raw.tb_consumos\")\n",
    "    .select(\"tipo_servico\", \"descricao\")\n",
    "    .dropDuplicates()\n",
    "    .withColumn(\n",
    "        \"pk_servico\",\n",
    "        F.sha2(F.concat_ws(\"||\", F.col(\"tipo_servico\"), F.col(\"descricao\")), 256)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Último surrogate key (sk_servico) ---\n",
    "last_id = df_dim_existente.agg(F.max(\"sk_servico\")).collect()[0][0]\n",
    "if last_id is None:\n",
    "    last_id = 0\n",
    "\n",
    "# --- Prepara registros de entrada com nova surrogate key ---\n",
    "window = Window.orderBy(\"tipo_servico\", \"descricao\")\n",
    "df_prepared = (\n",
    "    df_consumos.withColumn(\n",
    "        \"sk_servico\", (F.row_number().over(window) + last_id).cast(T.LongType())\n",
    "    )\n",
    "    .withColumn(\"start_date\", F.current_date())\n",
    "    .withColumn(\"update_date\", F.lit(None).cast(T.DateType()))\n",
    ")\n",
    "\n",
    "# --- Merge/upsert na dimensão ---\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_prepared.alias(\"source\"),\n",
    "    \"target.pk_servico = source.pk_servico\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"tipo_servico\": F.col(\"source.tipo_servico\"),\n",
    "        \"descricao\": F.col(\"source.descricao\"),\n",
    "        \"update_date\": F.current_date()\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"pk_servico\": F.col(\"source.pk_servico\"),\n",
    "        \"sk_servico\": F.col(\"source.sk_servico\"),\n",
    "        \"tipo_servico\": F.col(\"source.tipo_servico\"),\n",
    "        \"descricao\": F.col(\"source.descricao\"),\n",
    "        \"start_date\": F.col(\"source.start_date\"),\n",
    "        \"update_date\": F.col(\"source.update_date\")\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"Merge/upsert concluído na tabela:\", catalog_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de2fa8e-6464-47c4-a4ce-74f411e7fdc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from production.raw.tb_consumos\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2892966914182801,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "d_servicos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
